(window.webpackJsonp=window.webpackJsonp||[]).push([[41],{520:function(t,a,s){"use strict";s.r(a);var n=s(4),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[t._v("当网络过深的时候，会出现训练集上精度反而下降的情况，其原因是后向传播的时候出了问题，一些变量的信息在传递中丢失了，解决这个问题出现了下面两种方式")]),t._v(" "),s("h2",{attrs:{id:"residual-connection"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#residual-connection"}},[t._v("#")]),t._v(" Residual Connection")]),t._v(" "),s("p",[t._v("来源于ResNet, 将当前层网络输出和原始信息加起来，以便防止原始信息丢失，X2=H(x1) + x1。")]),t._v(" "),s("h2",{attrs:{id:"dense-connection"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#dense-connection"}},[t._v("#")]),t._v(" Dense Connection")]),t._v(" "),s("p",[t._v("来源于DenseNet拼接当层网络输出和原始输入，以便防止原始信息丢失，X2=[H(x1),x1]。\nResidual Connection可以保证维度，还可以加入门控机制，Dense Connection会增加维度，如何加有很多方式，需要去尝试。")]),t._v(" "),s("h2",{attrs:{id:"network-in-network"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#network-in-network"}},[t._v("#")]),t._v(" Network in Network")]),t._v(" "),s("p",[t._v("多个不同size的卷积核去提取不同维度的特征，最后进行拼接，一般与AutoMl（在某个层中尝试一些Network in Network,大的网络架构不变）结合，较为耗时")]),t._v(" "),s("h2",{attrs:{id:"gate"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#gate"}},[t._v("#")]),t._v(" gate")]),t._v(" "),s("p",[t._v("一般用sigmoid函数作为门控机制，来控制输入，输出，记忆等等。")]),t._v(" "),s("h2",{attrs:{id:"attention"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#attention"}},[t._v("#")]),t._v(" attention")]),t._v(" "),s("p",[t._v("通俗来说，attention机制就是，给定一组向量集合values，以及一个向量query，attention机制是一种根据该query计算values的加权求和的机制。google新定义的attention机制中定义Query,Key,Value,也就是通过计算query与各个key的内积，然后再进行softmax的方式来得到query和key对应的value的相似度，然后加权求和，key-value是一一对应的。比如query跟key的运算方式不一定是点乘（还可以是拼接后再内积一个参数向量），甚至权重都不一定要归一化，等等。attention的重点就是这个集合values中的每个value的“权值”的计算方法。")]),t._v(" "),s("ul",[s("li",[t._v("seq2seq中的attenion机制"),s("br"),t._v("\n在encoder的过程中保留每个RNN单元的隐藏状态（hidden state）得到（h1……hN），然后对于decoder的每一个timestep，因为有此时decoder的输入和上一步的隐藏状态输出，所以我们可以得到当前步的隐藏状态。假设第t步的（根据上一步隐藏状态输出与当前输入得到的）隐藏状态为St，在每个第t步利用St和hi进行dot点积得到attention score（ai），也称为“相似度“或者“影响度”，或者“匹配得分”，然后这里(a1,a2,....an)再做一个softmax转化成概率分布，就是对应的attention分布。")]),t._v(" "),s("li",[t._v("Multi-Head Attention"),s("br"),t._v("\n这个是Google提出的新概念，是Attention机制的完善。不过从形式上看，它其实就再简单不过了，就是把Q,K,V通过参数矩阵映射一下，然后再做Attention，把这个过程重复做hh次，结果拼接起来就行了，可谓“大道至简”了。具体来说就是最开始Q,K,V我们都不知道，但是可以用预测，即给定一个输入A，Q=Wq * A,K = Wk * A, V = Wv * A。headi=Attention(Q* Wqi,K* Wki,V* Wvi),     MultiHead(Q,K,V)=Concat(head1,...,headh) ，最后得到一个n × hd的序列。所谓“多头”（Multi-Head），就是只多做几次同样的事情（参数不共享），然后把结果拼接。所谓“多头”（Multi-Head），就是只多做几次同样的事情（参数不共享），然后把结果拼接。")]),t._v(" "),s("li",[t._v("self attention"),s("br"),t._v("\n到目前为止，对Attention层的描述都是一般化的，我们可以落实一些应用。比如，如果做阅读理解的话，Q可以是篇章的向量序列，取K=V为问题的向量序列，那么输出就是所谓的Aligned Question Embedding。所谓Self Attention，其实就是Attention(X,X,X)，X就是前面说的输入序列。也就是说，在序列内部做Attention，寻找序列内部的联系。"),s("br"),t._v("\n思想：Self attention也叫做intra-attention在没有任何额外信息的情况下，我们仍然可以通过允许句子使用 self attention机制来处理自己，从句子中提取关注信息。")]),t._v(" "),s("li",[t._v("Position Embedding "),s("br"),t._v("\n只要稍微思考一下就会发现，这样的模型并不能捕捉序列的顺序！换句话说，如果将K,V按行打乱顺序（相当于句子中的词序打乱），那么Attention的结果还是一样的。这就表明了，到目前为止，Attention模型顶多是一个非常精妙的“词袋模型”而已。这问题就比较严重了，大家知道，对于时间序列来说，尤其是对于NLP中的任务来说，顺序是很重要的信息，它代表着局部甚至是全局的结构，学习不到顺序信息，那么效果将会大打折扣（比如机器翻译中，有可能只把每个词都翻译出来了，但是不能组织成合理的句子）。于是Google再祭出了一招——Position Embedding，也就是“位置向量”，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了。")])]),t._v(" "),s("h2",{attrs:{id:"memory"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#memory"}},[t._v("#")]),t._v(" Memory")]),t._v(" "),s("p",[t._v("。。。。memory")]),t._v(" "),s("h2",{attrs:{id:"activation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#activation"}},[t._v("#")]),t._v(" activation")]),t._v(" "),s("p",[t._v("常见的sigmoid，tanh，relu"),s("br"),t._v("\n比较新的有swish，mish，gelu"),s("br"),t._v("\n当出现梯度爆炸的时候，用Gradient clipping 可能有用，修剪梯度。在torch当中可以直接调在loss.backward()和optimizer.step()之间调用torch.nn.clip_grad_value(),常见的思想是根据值做clip，这个值需要自己尝试（如梯度大于5强行改成5，小于-5强行改成-5等等），还有一种是根据所谓的norm去修剪梯度，不建议用norm去修剪，因为可能某些层出现了梯度爆炸，但有些层没有出现梯度爆炸，结果修剪梯度过后可能导致那部分梯度失效了。")]),t._v(" "),s("h2",{attrs:{id:"batch-normalization"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#batch-normalization"}},[t._v("#")]),t._v(" Batch Normalization")]),t._v(" "),s("p",[t._v("当我们用relu函数的时候，最希望算出来的结果值在0附近，这样的话激活函数效果更好，但是往往有偏差，在层数多了以后就会有很大的偏差，因此需要Batch Normalization，因此应该假设两个参数，是真实分布的均值和标准差，因为真实的均值和方差我们是不知道的，因此只能当作参数去训练，因此它是一个可学习、有参数的网络层。\n训练："),s("br"),t._v("\n1.计算样本均值。"),s("br"),t._v("\n2.计算样本方差。"),s("br"),t._v("\n3.样本数据标准化处理（利用前面的均值和方差来变换输入X，得到下文中BN层的输入Xk）。"),s("br"),t._v("\n4.进行平移和缩放处理。引入了这个可学习重构参数γ、β，（Y= Xk* γ + β）让我们的网络可以学习恢复出原始网络所要学习的特征分布。"),s("br"),t._v("\n5. 在反向传播时也会优化γ、β等参数"),s("br"),t._v("\n测试："),s("br"),t._v("\n测试的时候BN层直接使用训练好的参数γ、β，然后直接进行变化Y= X* γ + β即可。这里γ相当于训练数据的全局标准差、β相当于训练数据的全局均值，因此当网络有BN层和dropout层时，在测试时，必须调用model.eval()，这样才会使用固定的参数，dropout同理就不会抛弃神经元。")]),t._v(" "),s("ol",[s("li",[t._v("根据Batch Normalization的初衷，BN层要放在激活函数前，但是后来发现放在激活函数后效果更好，一般现在都放在激活函数后。")]),t._v(" "),s("li",[t._v("一般来说，dropout会将一部分神经元抛弃，这样对于我们计算BN层均值和标准差是有很大的影响的，所以现在一般BN层放在dropout前。也不一定，也有说BN和dropout一起用可能影响网络精度，具体需要尝试。")])]),t._v(" "),s("h2",{attrs:{id:"其它normalization"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#其它normalization"}},[t._v("#")]),t._v(" 其它normalization")]),t._v(" "),s("ul",[s("li",[t._v("layer normalization"),s("br"),t._v("\n由于batch一般较小，其均值和方差误差较大，因此有一种Layer Normalization认为同一层的神经元可能有相同的均值和方差。")]),t._v(" "),s("li",[t._v("Group Normalization"),s("br"),t._v("\n认为可能神经元某些维度上的均值方差不一样，因此只对某特定的维度做BN。")])]),t._v(" "),s("h2",{attrs:{id:"初始化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#初始化"}},[t._v("#")]),t._v(" 初始化")]),t._v(" "),s("p",[t._v("大类有根据常数的初始化和随机初始化，常数初始化通常效果不佳，太固定了。随机初始化一般是均匀分布和正态分布。")]),t._v(" "),s("ul",[s("li",[t._v("Xavier初始化和Kaiming 初始化"),s("br"),t._v("\n大体思想是认为神经网络的每一层方差大致不变，因此如果某一层神经元个数偏少，则有可能方偏小，则需要在初始化进行弥补。")])]),t._v(" "),s("p",[t._v("自定义初始化可以定义一个函数，举个栗子：")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("my_mlp "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" MLP"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#实例化一个MLP")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("weights_init_uniform")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("m"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n   classname "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" m"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__class__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__name__\n   "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" classname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("find"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Linear'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#比如只对线性层初始化")]),t._v("\n        m"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uniform_"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        m"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bias"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fill_"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmy_mlp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("apply")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("weights_init_uniform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br")])]),s("h2",{attrs:{id:"学习率"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#学习率"}},[t._v("#")]),t._v(" 学习率")]),t._v(" "),s("ul",[s("li",[t._v("常见trick：")])]),t._v(" "),s("ol",[s("li",[t._v("首先寻找ok的学习率，然后再调整其它参数")]),t._v(" "),s("li",[t._v("不同层采用不同的学习率(上下层训练程度不一样就采用不同学习率)")]),t._v(" "),s("li",[t._v("在最终阶段降低学习率，或者Babysit（要去观测整个训练，当发现网络在验证集上的精度已经不再提升了就开始逐步降低学习率）")])]),t._v(" "),s("ul",[s("li",[s("p",[t._v("常见的学习率"),s("br"),t._v("\nFinetune: 1e-5,2e-5,5e-5"),s("br"),t._v("\n重新训练，没有公认的介定，一般从0.01开始尝试")])]),t._v(" "),s("li",[s("p",[t._v("warm up"),s("br"),t._v("\n由于刚开始训练的时候学习率太大会导致不稳定，但是有最新论文提出刚开始如果学习率太小也会不稳定，因此使用一个折中的办法warm up。warm up是学习率从小到大再到小。"),s("br"),t._v("\n学习率设定的栗子：")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("  optimizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" optim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Adam"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'params'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("mlp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("first_layer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'lr'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                   "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'params'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("mlp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("second_layer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'lr'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("lr "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# adm optimizer当中也是有信息的，如果需要连续训练，则也需要保存再读取，但是读取厚还是需要重新设置一下学习率")]),t._v("\n   torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("state_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'optimizer.pt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n   optimizer2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" optim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Adam"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'params'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("mlp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("first_layer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'lr'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                   "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'params'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("mlp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("second_layer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'lr'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("lr "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n   optimizer2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_state_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'optimizer.pt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("          \n   optimizer2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__dict__\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br")])])])]),t._v(" "),s("p",[t._v("warm up的栗子：")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("optimizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" optim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Adam"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("params "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mlp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("lr "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\nn_epoch "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\nglobal_steps "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\nwarm_up_steps "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),t._v("\nmax_learninig_rate "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" epoch "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_epoch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n     "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" batch "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" dataloader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n     global_steps "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n     optimizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zero_grad"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n     x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" batch\n     predictions "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mlp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("squeeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n     loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" criterion"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("predictions"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n     loss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n     optimizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("step"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n     "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" global_steps "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n         optimizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("param_groups"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'lr'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" global_steps"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" max_learning_rate"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("warm_up_steps\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n         optimizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("param_groups"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'lr'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" max_learning_rate"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这里大于1000步不变，实际也可以使学习率再降低")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br"),s("span",{staticClass:"line-number"},[t._v("14")]),s("br"),s("span",{staticClass:"line-number"},[t._v("15")]),s("br"),s("span",{staticClass:"line-number"},[t._v("16")]),s("br"),s("span",{staticClass:"line-number"},[t._v("17")]),s("br"),s("span",{staticClass:"line-number"},[t._v("18")]),s("br"),s("span",{staticClass:"line-number"},[t._v("19")]),s("br")])]),s("h2",{attrs:{id:"梯度累积"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#梯度累积"}},[t._v("#")]),t._v(" 梯度累积")]),t._v(" "),s("p",[t._v("当显存不够时，不能用较大的batchsize。"),s("br"),t._v("\n总结来说：梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空，不断累加，累加一定次数后，根据累加的梯度更新网络参数，然后清空梯度，进行下一次循环。一定条件下，batchsize越大训练效果越好，梯度累加则实现了batchsize的变相扩大，如果accumulation_steps为8，则batchsize '变相' 扩大了8倍，是我们这种乞丐实验室解决显存受限的一个不错的trick，使用时需要注意，学习率也要适当放大。")]),t._v(" "),s("h2",{attrs:{id:"分布式训练"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#分布式训练"}},[t._v("#")]),t._v(" 分布式训练")]),t._v(" "),s("p",[t._v("多卡，传输数据")]),t._v(" "),s("h2",{attrs:{id:"半精度训练"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#半精度训练"}},[t._v("#")]),t._v(" 半精度训练")]),t._v(" "),s("p",[t._v("float16")])])}),[],!1,null,null,null);a.default=e.exports}}]);