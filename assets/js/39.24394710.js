(window.webpackJsonp=window.webpackJsonp||[]).push([[39],{519:function(t,e,r){"use strict";r.r(e);var n=r(4),i=Object(n.a)({},(function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h2",{attrs:{id:"深度迁移学习"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#深度迁移学习"}},[t._v("#")]),t._v(" 深度迁移学习")]),t._v(" "),r("ul",[r("li",[t._v("目的\n"),r("ul",[r("li",[t._v("解决word Embedding的不足， word Embedding最大的不足是无法解决不同词在不同的语境中编码一致的问题")]),t._v(" "),r("li",[t._v("充分利用无标注数据 （希望不需要人去标注）")]),t._v(" "),r("li",[t._v("使用较深的模型 （之前NLP的网络不会太深）")]),t._v(" "),r("li",[t._v("训练的时候用更少的数据（从其它任务迁移出一部分特征作为预训练）")])])]),t._v(" "),r("li",[t._v("ELMo"),r("br"),t._v("\n大致是先经过character embedding，再经过convolution，最终输入到一个双向LSTM中，其用法主要是拿出LSTM中的隐藏层，然后与word embedding去作拼接或求和等再输入到原本需要用到word embedding的位置"),r("br"),t._v("\n解决了前面三个目的（除了不能使用较深的模型）。")]),t._v(" "),r("li",[t._v("BERT"),r("br"),t._v("\n双向训练，与双向lstm的双向不同，lstm的双向交互其实在最后面的正向结果和反向结果交互，而bert的双向是在训练过程中就交互了，通过mask，然后让其前面的词和后面的词都参与预测，这才是bert的双向"),r("br"),t._v("\n两个任务，一个预测被mask的词，还有一个是NSR，分句，就是区分不同的句子。"),r("br"),t._v("\n输入有三个 input_ids,token_type_ids,position_ids。"),r("br"),t._v("\n给出一个demo，使用bertModel，然后用的数据集是IMDB数据集。"),r("a",{attrs:{href:"https://github.com/gq15760172077/pytorch/blob/master/bert-imdb.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("github链接"),r("OutboundLink")],1)])])])}),[],!1,null,null,null);e.default=i.exports}}]);