(window.webpackJsonp=window.webpackJsonp||[]).push([[38],{518:function(t,s,a){"use strict";a.r(s);var n=a(4),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("ul",[a("li",[t._v("torchtext,torchvision,torchaudio分别是torch提供的分别关于自然语言处理，视觉以及语音方面的三个库，包含了一些相关的数据集，可以直接下载。")]),t._v(" "),a("li",[t._v("NLTK（Natural language toolkit）是常见的自然语言处理工具包，包含了一些分词模块，如（‘punk’）, 其中nltk_data不好下载，可以通过离线的方式进行下载然后放置对应目录。")]),t._v(" "),a("li",[t._v("torchteext采用声明式方法加载数据，需要先声明一个Field对象，这个Field对象指定你想要怎么处理某个数据,each Field has its own Vocab class。")])]),t._v(" "),a("hr"),t._v(" "),a("h2",{attrs:{id:"field相关参数如下"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#field相关参数如下"}},[t._v("#")]),t._v(" Field相关参数如下：")]),t._v(" "),a("blockquote",[a("p",[t._v("squential：数据是否为序列数据，默认为Ture。如果为False，则不能使用分词。"),a("br"),t._v("\nuse_vocab：是否使用词典，默认为True。如果为False，那么输入的数据类型必须是数值类型(即使用vocab转换后的)。"),a("br"),t._v("\ninit_token：文本的其实字符，默认为None。"),a("br"),t._v("\neos_token：文本的结束字符，默认为None。"),a("br"),t._v("\nfix_length：所有样本的长度，不够则使用pad_token补全。默认为None，表示灵活长度。"),a("br"),t._v("\ntensor_type：把数据转换成的tensor类型 默认值为torch.LongTensor。"),a("br"),t._v("\npreprocessing：预处理pipeline， 用于分词之后、数值化之前，默认值为None。"),a("br"),t._v("\npostprocessing：后处理pipeline，用于数值化之后、转换为tensor之前，默认为None。"),a("br"),t._v("\nlower：是否把数据转换为小写，默认为False；"),a("br"),t._v("\ntokenize：分词函数，默认为str.split"),a("br"),t._v("\ninclude_lengths：是否返回一个已经补全的最小batch的元组和和一个包含每条数据长度的列表，默认值为False。"),a("br"),t._v("\nbatch_first：batch作为第一个维度；"),a("br"),t._v("\npad_token：用于补全的字符，默认为"),a("pad",[t._v("。"),a("br"),t._v("\nunk_token：替换袋外词的字符，默认为"),a("unk",[t._v("。"),a("br"),t._v("\npad_first：是否从句子的开头进行补全，默认为False；"),a("br"),t._v("\ntruncate_first：是否从句子的开头截断句子，默认为False；"),a("br"),t._v("\nstop_words：停用词；")])],1)],1)]),t._v(" "),a("ul",[a("li",[t._v("Tokenize")])]),t._v(" "),a("blockquote",[a("p",[t._v("Field 中的参数tokenize必须是一个函数，其作用是给定一个字符串，该函数以列表的形式返回分词的结果。一般常用的可以使用nltk.tokenize中的word_tokenize。")])]),t._v(" "),a("ul",[a("li",[t._v("Vocab")])]),t._v(" "),a("blockquote",[a("p",[t._v("Field对象可以通过调用build_vocab（）方法来生成一个内置的Vocab对象。build的时候可以指定MAX_VOCAB_SIZE，以及预训练好的vectors，")])]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TEXT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vocab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("freqs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# freqs是一个Counter对象，包含了词表中单词的计数信息")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TEXT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vocab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("freqs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'at'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TEXT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vocab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("itos"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# itos表示index to str")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TEXT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vocab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stoi"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<unk>'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# stoi表示str to index")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TEXT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vocab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unk_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TEXT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vocab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vectors"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 词向量")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("p",[a("strong",[t._v("举个栗子")])]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("TEXT "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tokenize "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" include_lengths "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nLABEL "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LabelField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dtype "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrain_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" test_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("IMDB"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("splits"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TEXT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" LABEL"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrain_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("random_state "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("seed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("SEED"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\nMAX_VOCAB_SIZE "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("25000")]),t._v("\nTEXT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("build_vocab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("       \n                        max_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" MAX_VOCAB_SIZE"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("      \n                        vectors "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"glove.6B.300d"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("    \n                        unk_init "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("normal_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                        LABEL"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("build_vocab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br")])]),a("h2",{attrs:{id:"dataloader"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dataloader"}},[t._v("#")]),t._v(" DataLoader")]),t._v(" "),a("p",[t._v("DataLoader 是 torch.utils.data下的工具包，用于方便训练或测试的时候数据迭代。")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("transform"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("transforms"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Compose"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("transforms"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ToTensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("              transforms"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Normalize"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("                                                     "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrainset "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torchvision"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CIFAR10"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./data'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \ndownload"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("transform"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("transform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrainloader "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataLoader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("trainset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("shuffle"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("num_workers"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntestset "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torchvision"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CIFAR10"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./data'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("download"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("transform"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("transform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntestloader "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataLoader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("testset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("shuffle"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("num_workers"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("p",[t._v("也可以用BucketIterator代替，BucketIterator 是torch.legacy.data下的包")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("train_iterator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vaild_iterator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" test_iterator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BucketIterator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("splits"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("   \n                                "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" test_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("   \n                                batch_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" BATCH_SIZE"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                                sort_within_batch "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  \n                                device "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" device\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("h2",{attrs:{id:"lstm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lstm"}},[t._v("#")]),t._v(" LSTM")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("LSTM"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embedding_dim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# embdding的维度")]),t._v("\n            hidden_dim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("          \n            num_layers"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("n_layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# LSTM的层数，一般不超过三层")]),t._v("\n            bidirectional"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("bidirectional"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#是否双向LSTM ，True-> 是    ")]),t._v("\n            dropout "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dropout\n            "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("p",[t._v("注意：当bidirectional为True的时候喂入数据的时候必须使用pack_padded_sequence和pad_packed_sequence，\n这两个函数位于rorch.nn.utils.rnn下。其主要作用是将三维的输入数据去掉padding后变为二维，同时再加上对应的batch_size，这个batch_size是对应的数据中未填充padding的词个数。还要注意pack_padded_sequence输入的lengths必须降序排列。\n这个可以在构造dataloader迭代器的时候就完成。（将batch这一维度去掉，从而拼接成二维数据，再用一个数组batch_size表示每个句子中有多少个时间步（即不考虑padding的词，这样可以减少对面padding的运算））")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("packed_embedded "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rnn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pack_padded_sequence"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("embedded"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text_lengths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npacked_output"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hidden"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cell"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rnn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("packed_embedded"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\noutput"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" output_lengths "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rnn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pad_packed_sequence"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("packed_output"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br")])]),a("p",[t._v("注意LSTM中有一个参数叫batch_first，当它为True的时候，batch_size在第一个维度。默认为False,因此batch_size一般位于第二个维度，即常规的LSTM输入数据的维度应该是[seq_len, batch_size,input_size],其中input_size一般是输入的时候的embedding_dim，喂入数据的时候需要注意。")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("randn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 可以理解为：batch_size = false的时候， 1个batch中有3个句子，每个句子5个单词，每个单词用10维的向量表示；而句子的长度是不一样的，所以seq_len可长可短，这也是LSTM可以解决长短序列的特殊之处。只有seq_len这一参数是可变的。")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("h2",{attrs:{id:"一个简单的文本分类项目"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#一个简单的文本分类项目"}},[t._v("#")]),t._v(" 一个简单的文本分类项目")]),t._v(" "),a("p",[t._v('数据集用imdb数据，用nltk中tokenize进行分词。词向量用的"glove.6B.300d"'),a("br"),t._v("\n1.使用pytorch自带的api："),a("a",{attrs:{href:"https://github.com/gq15760172077/pytorch/blob/master/basicTextClassified.py",target:"_blank",rel:"noopener noreferrer"}},[t._v("github链接1"),a("OutboundLink")],1),a("br"),t._v("\n2.自己处理数据："),a("a",{attrs:{href:"https://github.com/gq15760172077/pytorch/blob/master/gru-classify.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("github链接2"),a("OutboundLink")],1)])])}),[],!1,null,null,null);s.default=e.exports}}]);