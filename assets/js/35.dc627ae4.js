(window.webpackJsonp=window.webpackJsonp||[]).push([[35],{514:function(t,s,n){"use strict";n.r(s);var a=n(4),e=Object(a.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h2",{attrs:{id:"结构化数据挖掘"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#结构化数据挖掘"}},[t._v("#")]),t._v(" 结构化数据挖掘")]),t._v(" "),n("ul",[n("li",[t._v("结构化数据就是指表格数据")])]),t._v(" "),n("blockquote",[n("p",[t._v("每列是一个变量，可离散也可连续。\n结构化数据占大多数，非结构化数据可以转化为结构化数据。")])]),t._v(" "),n("ul",[n("li",[t._v("传统的结构化数据")])]),t._v(" "),n("blockquote",[n("p",[t._v("取决于业务理解\n探索性分析是常态\n90%时间花费在清洗和探索性分析上")])]),t._v(" "),n("ul",[n("li",[t._v("一些实际问题")])]),t._v(" "),n("blockquote",[n("p",[t._v("高维稀疏变量（影响一个问题的因素是多维的）\n较差的变量质量")])]),t._v(" "),n("ul",[n("li",[t._v("pandas"),n("br"),t._v("\nset_index 也可以设置成多层索引，这个是对于行的索引，行的索引用loc或者iloc\nMultiIndex 也可以设置成多层索引，这个是对于列的索引，列的索引可以直接df[][]选取")])]),t._v(" "),n("div",{staticClass:"language-python line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("df "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n     "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n     "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n     "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("11")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rename_axis"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'b'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("columns "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MultiIndex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_tuples"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n   "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'c'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'e'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'c'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'f'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" names"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'level_1'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'level_2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# df['c','e']")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# df.iloc[2]")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br"),n("span",{staticClass:"line-number"},[t._v("8")]),n("br"),n("span",{staticClass:"line-number"},[t._v("9")]),n("br"),n("span",{staticClass:"line-number"},[t._v("10")]),n("br")])]),n("ul",[n("li",[n("p",[t._v("matplotlib.pyplot"),n("br"),t._v("\n直方图，plt.hist(data,num_bins,Density =True)"),n("br"),t._v("\n散点图，plt.plot(x,y)"),n("br"),t._v("\nboxplot, plt.boxplot(data1,data2))，比较两组数据media，25%,75%，以及异常值")])]),t._v(" "),n("li",[n("p",[t._v("Target Mean Encoding"),n("br"),t._v("\n用这种方式编码，可以减少维度，如三分类需要增加三维数据，target-mean只需要增加一维。用对应组的目标均值来作为编码\ndf.groupby([A, B])[C].agg(func)")])]),t._v(" "),n("li",[n("p",[t._v("categorical encoder\npip install category_encoders,包括以下15种编码方法")])])]),t._v(" "),n("blockquote",[n("p",[t._v("Backward Difference Coding"),n("br"),t._v("\nBaseN"),n("br"),t._v("\nBinaryCatBoost Encoder"),n("br"),t._v("\nHashingHelmert Coding"),n("br"),t._v("\nJames-Stein Encoder"),n("br"),t._v("\nLeave One Out"),n("br"),t._v("\nM-estimate"),n("br"),t._v("\nOne Hot"),n("br"),t._v("\nOrdinal"),n("br"),t._v("\nPolynomial Coding"),n("br"),t._v("\nSum Coding"),n("br"),t._v("\nTarget Encoder\nWeight of Evidence")])]),t._v(" "),n("div",{staticClass:"language-python line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" category_encoders "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" ce\nencoder "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ce"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BackwardDifferenceEncoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cols"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nencoder "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ce"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BaseNEncoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cols"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nencoder "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ce"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BinaryEncoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cols"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nencoder "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ce"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CatBoostEncoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cols"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nencoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nX_cleaned "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" encoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X_dirty"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\ndf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ID'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                   "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'RATING'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'G'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'B'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'G'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'B'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'B'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'G'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \nencoder "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ce"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BinaryEncoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cols"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'RATING'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 编码数据")]),t._v("\ndf_transform "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" encoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br"),n("span",{staticClass:"line-number"},[t._v("2")]),n("br"),n("span",{staticClass:"line-number"},[t._v("3")]),n("br"),n("span",{staticClass:"line-number"},[t._v("4")]),n("br"),n("span",{staticClass:"line-number"},[t._v("5")]),n("br"),n("span",{staticClass:"line-number"},[t._v("6")]),n("br"),n("span",{staticClass:"line-number"},[t._v("7")]),n("br"),n("span",{staticClass:"line-number"},[t._v("8")]),n("br"),n("span",{staticClass:"line-number"},[t._v("9")]),n("br"),n("span",{staticClass:"line-number"},[t._v("10")]),n("br"),n("span",{staticClass:"line-number"},[t._v("11")]),n("br"),n("span",{staticClass:"line-number"},[t._v("12")]),n("br"),n("span",{staticClass:"line-number"},[t._v("13")]),n("br"),n("span",{staticClass:"line-number"},[t._v("14")]),n("br")])]),n("h2",{attrs:{id:"连续变量离散化"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#连续变量离散化"}},[t._v("#")]),t._v(" 连续变量离散化")]),t._v(" "),n("ol",[n("li",[t._v("uniform")]),t._v(" "),n("li",[t._v("quantile（分位数，先排序，再分类）")])]),t._v(" "),n("div",{staticClass:"language-python line-numbers-mode"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("   DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("quantile"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("q"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" numeric_only"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" interpolation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("’linear’"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),n("div",{staticClass:"line-numbers-wrapper"},[n("span",{staticClass:"line-number"},[t._v("1")]),n("br")])]),n("ol",{attrs:{start:"3"}},[n("li",[t._v("基于聚类,无监督\nkmeans,")]),t._v(" "),n("li",[t._v("基于树")])]),t._v(" "),n("ul",[n("li",[t._v("entity embedding"),n("br"),t._v("\none hot 乘以一个矩阵的形式，一般应用于离散变量的embedding，对于连续变量，其离散化必然会丢失一些东西，但也可以通过某种方式加回来。")]),t._v(" "),n("li",[t._v("连续变量embedding的一些trick\n"),n("ol",[n("li",[t._v("首先对连续变量的取值范围进行分组，如1-10000分20组，numberOfBins,")]),t._v(" "),n("li",[t._v("计算每个组中对应的Centroid（用Ci表示），这里Ci可以是平均值或者median。")]),t._v(" "),n("li",[t._v("对于输入X，计算其相对于每个分组的权重$W_{i}$，$\\epsilon$为一个非常小的常数，防止分母为0。"),n("br"),t._v("\n$$W_{i} = softmax(\\frac{1}{\\left | X - C_{i} \\right|  + \\epsilon})$$"),n("br"),t._v("\n4.根据对应权重乘以对应组的$E_{i}$（embedding vector）即可算出当前X的词向量。"),n("br"),t._v("\n$$V = \\sum_{i=1}^N W_{i} * E_{i}$$"),n("br"),t._v("\n代码实现："),n("a",{attrs:{href:"https://github.com/gq15760172077/pytorch/blob/master/Tutorials%20on%20Entity%20Embedding.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("github链接"),n("OutboundLink")],1)])])])])])}),[],!1,null,null,null);s.default=e.exports}}]);