(window.webpackJsonp=window.webpackJsonp||[]).push([[46],{525:function(a,t,s){"use strict";s.r(t);var e=s(4),n=Object(e.a)({},(function(){var a=this,t=a.$createElement,s=a._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[s("h2",{attrs:{id:"meta-learning-是最近比较火的一个研究方向-这里大致记录一下其思想。"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#meta-learning-是最近比较火的一个研究方向-这里大致记录一下其思想。"}},[a._v("#")]),a._v(" meta learning 是最近比较火的一个研究方向，这里大致记录一下其思想。")]),a._v(" "),s("h2",{attrs:{id:"思想"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#思想"}},[a._v("#")]),a._v(" 思想")]),a._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[a._v("元学习Meta Learning，含义为学会学习，即learn to learn，就是带着这种对人类这种“学习能力”的期望诞生的。Meta Learning希望使得模型获取一种“学会学习”的能力，使其可以在获取已有“知识”的基础上快速学习新的任务，如：让Alphago迅速学会下象棋，让一个猫咪图片分类器，迅速具有分类其他物体的能力。通常在机器学习里，我们会使用某个场景的大量数据来训练模型；然而当场景发生改变，模型就需要重新训练。但是对于人类而言，一个小朋友成长过程中会见过许多物体的照片，某一天，当Ta（第一次）仅仅看了几张狗的照片，就可以很好地对狗和其他物体进行区分。  \n")])])]),s("h2",{attrs:{id:"与传统的机器学习不同的地方"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#与传统的机器学习不同的地方"}},[a._v("#")]),a._v(" 与传统的机器学习不同的地方")]),a._v(" "),s("ul",[s("li",[a._v("传统的机器学习是为了求解某个具体的task或者多个task的最优解，其输入是某个task的训练集，验证集，测试集。在机器学习中，训练单位是一条数据，通过数据来对模型进行优化；数据可以分为训练集、测试集和验证集。在元学习中，训练单位分层级了，第一层训练单位是任务，也就是说，元学习中要准备许多任务来进行学习，第二层训练单位才是每个任务对应的数据。")]),a._v(" "),s("li",[a._v("meta learning的输入可以理解是多个任务，这些任务之间是带有一定相关性的，满足某种分布。同时其可以直接应用于其它任务上，使得其它任务训练得更快更好（如果目标任务数据集很少的话，则可以很好的解决这个问题）。它的输出是一个模型或者说函数，它的输出就能用于其它任务。")]),a._v(" "),s("li",[a._v("二者的目的都是找一个Function，只是两个Function的功能不同，要做的事情不一样。机器学习中的Function直接作用于特征和标签，去寻找特征与标签之间的关联；而元学习中的Function是用于寻找新的f，新的f才会应用于具体的任务。")]),a._v(" "),s("li",[a._v("meta learning其原则是可以自己构建新的模型和初始化参数，但是构建新模型的方式太过复杂了，我还没有看，下面会举个比较基础的算法（MAML）来构建新的初始化参数。")])]),a._v(" "),s("h2",{attrs:{id:"loss"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#loss"}},[a._v("#")]),a._v(" loss")]),a._v(" "),s("p",[a._v("我们需要注意下meta learning的loss与传统的机器学习loss不同。")]),a._v(" "),s("img",{attrs:{src:a.$withBase("/notes/meta1.png"),alt:"meta1"}}),a._v(" "),s("p",[s("strong",[a._v("这里用公式不太好表述，N代表N个任务，l表示loss，Φ表示当前模型的参数，θ^n 表示在第n个任务上的参数，什么意思呢？")]),s("br"),a._v("\n其大致思想就是说meta learning不关注某个具体任务上的训练集loss，比如我现在有很多分类任务，如区分猫狗，区分非机动车与机动车，区分人与动物等等任务，当前模型参数是Φ，我随机sample N个任务，用Φ去当作 模型起始参数，在每个任务上分别训练（其原论文说的训练一次，具体实现时又不是一次，所以暂不纠结这个）几次，然后去验证集上计算loss，将这个loss反向传播到meta learning模型中去，然后再累积多任务求和更新Φ，从这里就可以看出来这个Φ不一定在每个问题上都是最优的，但是其目的是想得到一个Φ，然后应用于某个具体任务时，只需要训练少量的step就可以使其达到最优，这点也是和transfer learning 区别最大的地方。")]),a._v(" "),s("h2",{attrs:{id:"maml"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#maml"}},[a._v("#")]),a._v(" MAML")]),a._v(" "),s("ul",[s("li",[a._v("MAML的目的是获取一组更好的模型初始化参数（即让模型自己学会初始化）。我们通过许多训练任务进行元学习的训练，使得模型学习到“先验知识”（初始化的参数）。"),s("br"),a._v("\n其算法流程图如下：其思想和上面叙述的差不多。\n"),s("img",{attrs:{src:a.$withBase("/notes/meta2.png"),alt:"meta2"}})])])])}),[],!1,null,null,null);t.default=n.exports}}]);