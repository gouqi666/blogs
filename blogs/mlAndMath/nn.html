<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>神经网络基础总结 | richard&#39;s blog</title>
    <meta name="generator" content="VuePress 1.5.4">
    <link rel="icon" href="/favicon.ico">
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    <link rel="preload" href="/assets/css/0.styles.1157b0c6.css" as="style"><link rel="preload" href="/assets/js/app.c903a5b0.js" as="script"><link rel="preload" href="/assets/js/3.20f81737.js" as="script"><link rel="preload" href="/assets/js/1.0bcafa9d.js" as="script"><link rel="preload" href="/assets/js/41.3b6c6e38.js" as="script"><link rel="prefetch" href="/assets/js/10.6b96f5d4.js"><link rel="prefetch" href="/assets/js/11.a32f71e4.js"><link rel="prefetch" href="/assets/js/12.8e6a11be.js"><link rel="prefetch" href="/assets/js/13.3ecd83ac.js"><link rel="prefetch" href="/assets/js/14.8f9319d1.js"><link rel="prefetch" href="/assets/js/15.9e6a2cb7.js"><link rel="prefetch" href="/assets/js/16.bf340180.js"><link rel="prefetch" href="/assets/js/17.5040d859.js"><link rel="prefetch" href="/assets/js/18.4502d066.js"><link rel="prefetch" href="/assets/js/19.e52ca789.js"><link rel="prefetch" href="/assets/js/20.0346bf4f.js"><link rel="prefetch" href="/assets/js/21.acd487f8.js"><link rel="prefetch" href="/assets/js/22.560692a6.js"><link rel="prefetch" href="/assets/js/23.bbb61a77.js"><link rel="prefetch" href="/assets/js/24.7c693121.js"><link rel="prefetch" href="/assets/js/25.88f33223.js"><link rel="prefetch" href="/assets/js/26.761eab8a.js"><link rel="prefetch" href="/assets/js/27.d6399fc9.js"><link rel="prefetch" href="/assets/js/28.d18aae33.js"><link rel="prefetch" href="/assets/js/29.dd5e888c.js"><link rel="prefetch" href="/assets/js/30.8f026d79.js"><link rel="prefetch" href="/assets/js/31.19875b23.js"><link rel="prefetch" href="/assets/js/32.78bff936.js"><link rel="prefetch" href="/assets/js/33.a969ce41.js"><link rel="prefetch" href="/assets/js/34.1842c6d5.js"><link rel="prefetch" href="/assets/js/35.2d6864ee.js"><link rel="prefetch" href="/assets/js/36.000c1290.js"><link rel="prefetch" href="/assets/js/37.9d1da8b7.js"><link rel="prefetch" href="/assets/js/38.7cf38d7e.js"><link rel="prefetch" href="/assets/js/39.dd873bdc.js"><link rel="prefetch" href="/assets/js/4.f02b9190.js"><link rel="prefetch" href="/assets/js/40.dddc961e.js"><link rel="prefetch" href="/assets/js/42.1935f257.js"><link rel="prefetch" href="/assets/js/43.b1b19eba.js"><link rel="prefetch" href="/assets/js/44.0b359d11.js"><link rel="prefetch" href="/assets/js/45.b58274f7.js"><link rel="prefetch" href="/assets/js/46.e27ac301.js"><link rel="prefetch" href="/assets/js/47.838f2053.js"><link rel="prefetch" href="/assets/js/48.58309296.js"><link rel="prefetch" href="/assets/js/49.6a602edf.js"><link rel="prefetch" href="/assets/js/5.a334d371.js"><link rel="prefetch" href="/assets/js/50.f383e87c.js"><link rel="prefetch" href="/assets/js/51.bd9877e7.js"><link rel="prefetch" href="/assets/js/52.3c82c076.js"><link rel="prefetch" href="/assets/js/53.29c221f4.js"><link rel="prefetch" href="/assets/js/54.e65c55b9.js"><link rel="prefetch" href="/assets/js/6.35e528d3.js"><link rel="prefetch" href="/assets/js/7.cac617fb.js"><link rel="prefetch" href="/assets/js/8.42a61663.js"><link rel="prefetch" href="/assets/js/9.851ca25f.js">
    <link rel="stylesheet" href="/assets/css/0.styles.1157b0c6.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-2d5f533b><div data-v-2d5f533b><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-2d5f533b data-v-2d5f533b><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-64685f0e data-v-2d5f533b data-v-2d5f533b><h3 class="title" style="display:none;" data-v-64685f0e data-v-64685f0e>richard's blog</h3> <!----> <label id="box" class="inputBox" style="display:none;" data-v-64685f0e data-v-64685f0e><input type="password" value="" data-v-64685f0e> <span data-v-64685f0e>Konck! Knock!</span> <button data-v-64685f0e>OK</button></label> <div class="footer" style="display:none;" data-v-64685f0e data-v-64685f0e><span data-v-64685f0e><i class="iconfont reco-theme" data-v-64685f0e></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-64685f0e>vuePress-theme-reco</a></span> <span data-v-64685f0e><i class="iconfont reco-copyright" data-v-64685f0e></i> <a data-v-64685f0e><span data-v-64685f0e>gouqi</span>
            
          <span data-v-64685f0e>2020 - </span>
          2023
        </a></span></div></div> <div class="hide" data-v-2d5f533b><header class="navbar" data-v-2d5f533b><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/logo.png" alt="richard's blog" class="logo"> <span class="site-name">richard's blog</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/algorithm/" class="nav-link"><i class="iconfont undefined"></i>
  algorithm
</a></li><li class="dropdown-item"><!----> <a href="/categories/essay/" class="nav-link"><i class="iconfont undefined"></i>
  essay
</a></li><li class="dropdown-item"><!----> <a href="/categories/mlAndMath/" class="nav-link"><i class="iconfont undefined"></i>
  mlAndMath
</a></li><li class="dropdown-item"><!----> <a href="/categories/notes/" class="nav-link"><i class="iconfont undefined"></i>
  notes
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/gq15760172077/blogs" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li><li class="dropdown-item"><!----> <a href="mailto:1249224822@qq.com" class="nav-link external"><i class="iconfont reco-mail"></i>
  E-mail
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li><li class="dropdown-item"><!----> <a href="http://wpa.qq.com/msgrd?v=3&amp;uin=1249224822&amp;site=qq&amp;menu=yes" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-qq"></i>
  qq
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-2d5f533b></div> <aside class="sidebar" data-v-2d5f533b><div class="personal-info-wrapper" data-v-ca798c94 data-v-2d5f533b><img src="/avatar.jpg" alt="author-avatar" class="personal-img" data-v-ca798c94> <h3 class="name" data-v-ca798c94>
    gouqi
  </h3> <div class="num" data-v-ca798c94><div data-v-ca798c94><h3 data-v-ca798c94>39</h3> <h6 data-v-ca798c94>Article</h6></div> <div data-v-ca798c94><h3 data-v-ca798c94>41</h3> <h6 data-v-ca798c94>Tag</h6></div></div> <hr data-v-ca798c94></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/algorithm/" class="nav-link"><i class="iconfont undefined"></i>
  algorithm
</a></li><li class="dropdown-item"><!----> <a href="/categories/essay/" class="nav-link"><i class="iconfont undefined"></i>
  essay
</a></li><li class="dropdown-item"><!----> <a href="/categories/mlAndMath/" class="nav-link"><i class="iconfont undefined"></i>
  mlAndMath
</a></li><li class="dropdown-item"><!----> <a href="/categories/notes/" class="nav-link"><i class="iconfont undefined"></i>
  notes
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/gq15760172077/blogs" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li><li class="dropdown-item"><!----> <a href="mailto:1249224822@qq.com" class="nav-link external"><i class="iconfont reco-mail"></i>
  E-mail
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li><li class="dropdown-item"><!----> <a href="http://wpa.qq.com/msgrd?v=3&amp;uin=1249224822&amp;site=qq&amp;menu=yes" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-qq"></i>
  qq
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-64685f0e data-v-2d5f533b><h3 class="title" style="display:none;" data-v-64685f0e data-v-64685f0e>神经网络基础总结</h3> <!----> <label id="box" class="inputBox" style="display:none;" data-v-64685f0e data-v-64685f0e><input type="password" value="" data-v-64685f0e> <span data-v-64685f0e>Konck! Knock!</span> <button data-v-64685f0e>OK</button></label> <div class="footer" style="display:none;" data-v-64685f0e data-v-64685f0e><span data-v-64685f0e><i class="iconfont reco-theme" data-v-64685f0e></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-64685f0e>vuePress-theme-reco</a></span> <span data-v-64685f0e><i class="iconfont reco-copyright" data-v-64685f0e></i> <a data-v-64685f0e><span data-v-64685f0e>gouqi</span>
            
          <span data-v-64685f0e>2020 - </span>
          2023
        </a></span></div></div> <div data-v-2d5f533b><main class="page"><div class="page-title" style="display:none;"><h1 class="title">神经网络基础总结</h1> <div data-v-3b7f5bdf><i class="iconfont reco-account" data-v-3b7f5bdf><span data-v-3b7f5bdf>gouqi</span></i> <i class="iconfont reco-date" data-v-3b7f5bdf><span data-v-3b7f5bdf>2021-04-06</span></i> <i class="iconfont reco-eye" data-v-3b7f5bdf><span id="/blogs/mlAndMath/nn.html" data-flag-title="Your Article Title" class="leancloud-visitors" data-v-3b7f5bdf><a class="leancloud-visitors-count" style="font-size:.9rem;font-weight:normal;color:#999;"></a></span></i> <i class="iconfont reco-tag tags" data-v-3b7f5bdf><span class="tag-item" data-v-3b7f5bdf>nn</span><span class="tag-item" data-v-3b7f5bdf>dl</span></i></div></div> <div class="theme-reco-content content__default" style="display:none;"><p>当网络过深的时候，会出现训练集上精度反而下降的情况，其原因是后向传播的时候出了问题，一些变量的信息在传递中丢失了，解决这个问题出现了下面两种方式</p> <h2 id="residual-connection"><a href="#residual-connection" class="header-anchor">#</a> Residual Connection</h2> <p>来源于ResNet, 将当前层网络输出和原始信息加起来，以便防止原始信息丢失，X2=H(x1) + x1。</p> <h2 id="dense-connection"><a href="#dense-connection" class="header-anchor">#</a> Dense Connection</h2> <p>来源于DenseNet拼接当层网络输出和原始输入，以便防止原始信息丢失，X2=[H(x1),x1]。
Residual Connection可以保证维度，还可以加入门控机制，Dense Connection会增加维度，如何加有很多方式，需要去尝试。</p> <h2 id="network-in-network"><a href="#network-in-network" class="header-anchor">#</a> Network in Network</h2> <p>多个不同size的卷积核去提取不同维度的特征，最后进行拼接，一般与AutoMl（在某个层中尝试一些Network in Network,大的网络架构不变）结合，较为耗时</p> <h2 id="gate"><a href="#gate" class="header-anchor">#</a> gate</h2> <p>一般用sigmoid函数作为门控机制，来控制输入，输出，记忆等等。</p> <h2 id="attention"><a href="#attention" class="header-anchor">#</a> attention</h2> <p>通俗来说，attention机制就是，给定一组向量集合values，以及一个向量query，attention机制是一种根据该query计算values的加权求和的机制。google新定义的attention机制中定义Query,Key,Value,也就是通过计算query与各个key的内积，然后再进行softmax的方式来得到query和key对应的value的相似度，然后加权求和，key-value是一一对应的。比如query跟key的运算方式不一定是点乘（还可以是拼接后再内积一个参数向量），甚至权重都不一定要归一化，等等。attention的重点就是这个集合values中的每个value的“权值”的计算方法。</p> <ul><li>seq2seq中的attenion机制<br>
在encoder的过程中保留每个RNN单元的隐藏状态（hidden state）得到（h1……hN），然后对于decoder的每一个timestep，因为有此时decoder的输入和上一步的隐藏状态输出，所以我们可以得到当前步的隐藏状态。假设第t步的（根据上一步隐藏状态输出与当前输入得到的）隐藏状态为St，在每个第t步利用St和hi进行dot点积得到attention score（ai），也称为“相似度“或者“影响度”，或者“匹配得分”，然后这里(a1,a2,....an)再做一个softmax转化成概率分布，就是对应的attention分布。</li> <li>Multi-Head Attention<br>
这个是Google提出的新概念，是Attention机制的完善。不过从形式上看，它其实就再简单不过了，就是把Q,K,V通过参数矩阵映射一下，然后再做Attention，把这个过程重复做hh次，结果拼接起来就行了，可谓“大道至简”了。具体来说就是最开始Q,K,V我们都不知道，但是可以用预测，即给定一个输入A，Q=Wq * A,K = Wk * A, V = Wv * A。headi=Attention(Q* Wqi,K* Wki,V* Wvi),     MultiHead(Q,K,V)=Concat(head1,...,headh) ，最后得到一个n × hd的序列。所谓“多头”（Multi-Head），就是只多做几次同样的事情（参数不共享），然后把结果拼接。所谓“多头”（Multi-Head），就是只多做几次同样的事情（参数不共享），然后把结果拼接。</li> <li>self attention<br>
到目前为止，对Attention层的描述都是一般化的，我们可以落实一些应用。比如，如果做阅读理解的话，Q可以是篇章的向量序列，取K=V为问题的向量序列，那么输出就是所谓的Aligned Question Embedding。所谓Self Attention，其实就是Attention(X,X,X)，X就是前面说的输入序列。也就是说，在序列内部做Attention，寻找序列内部的联系。<br>
思想：Self attention也叫做intra-attention在没有任何额外信息的情况下，我们仍然可以通过允许句子使用 self attention机制来处理自己，从句子中提取关注信息。</li> <li>Position Embedding <br>
只要稍微思考一下就会发现，这样的模型并不能捕捉序列的顺序！换句话说，如果将K,V按行打乱顺序（相当于句子中的词序打乱），那么Attention的结果还是一样的。这就表明了，到目前为止，Attention模型顶多是一个非常精妙的“词袋模型”而已。这问题就比较严重了，大家知道，对于时间序列来说，尤其是对于NLP中的任务来说，顺序是很重要的信息，它代表着局部甚至是全局的结构，学习不到顺序信息，那么效果将会大打折扣（比如机器翻译中，有可能只把每个词都翻译出来了，但是不能组织成合理的句子）。于是Google再祭出了一招——Position Embedding，也就是“位置向量”，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了。</li></ul> <h2 id="memory"><a href="#memory" class="header-anchor">#</a> Memory</h2> <p>。。。。memory</p> <h2 id="activation"><a href="#activation" class="header-anchor">#</a> activation</h2> <p>常见的sigmoid，tanh，relu<br>
比较新的有swish，mish，gelu<br>
当出现梯度爆炸的时候，用Gradient clipping 可能有用，修剪梯度。在torch当中可以直接调在loss.backward()和optimizer.step()之间调用torch.nn.clip_grad_value(),常见的思想是根据值做clip，这个值需要自己尝试（如梯度大于5强行改成5，小于-5强行改成-5等等），还有一种是根据所谓的norm去修剪梯度，不建议用norm去修剪，因为可能某些层出现了梯度爆炸，但有些层没有出现梯度爆炸，结果修剪梯度过后可能导致那部分梯度失效了。</p> <h2 id="batch-normalization"><a href="#batch-normalization" class="header-anchor">#</a> Batch Normalization</h2> <p>当我们用relu函数的时候，最希望算出来的结果值在0附近，这样的话激活函数效果更好，但是往往有偏差，在层数多了以后就会有很大的偏差，因此需要Batch Normalization，因此应该假设两个参数，是真实分布的均值和标准差，因为真实的均值和方差我们是不知道的，因此只能当作参数去训练，因此它是一个可学习、有参数的网络层。
训练：<br>
1.计算样本均值。<br>
2.计算样本方差。<br>
3.样本数据标准化处理（利用前面的均值和方差来变换输入X，得到下文中BN层的输入Xk）。<br>
4.进行平移和缩放处理。引入了这个可学习重构参数γ、β，（Y= Xk* γ + β）让我们的网络可以学习恢复出原始网络所要学习的特征分布。<br>
5. 在反向传播时也会优化γ、β等参数<br>
测试：<br>
测试的时候BN层直接使用训练好的参数γ、β，然后直接进行变化Y= X* γ + β即可。这里γ相当于训练数据的全局标准差、β相当于训练数据的全局均值，因此当网络有BN层和dropout层时，在测试时，必须调用model.eval()，这样才会使用固定的参数，dropout同理就不会抛弃神经元。</p> <ol><li>根据Batch Normalization的初衷，BN层要放在激活函数前，但是后来发现放在激活函数后效果更好，一般现在都放在激活函数后。</li> <li>一般来说，dropout会将一部分神经元抛弃，这样对于我们计算BN层均值和标准差是有很大的影响的，所以现在一般BN层放在dropout前。也不一定，也有说BN和dropout一起用可能影响网络精度，具体需要尝试。</li></ol> <h2 id="其它normalization"><a href="#其它normalization" class="header-anchor">#</a> 其它normalization</h2> <ul><li>layer normalization<br>
由于batch一般较小，其均值和方差误差较大，因此有一种Layer Normalization认为同一层的神经元可能有相同的均值和方差。</li> <li>Group Normalization<br>
认为可能神经元某些维度上的均值方差不一样，因此只对某特定的维度做BN。</li></ul> <h2 id="初始化"><a href="#初始化" class="header-anchor">#</a> 初始化</h2> <p>大类有根据常数的初始化和随机初始化，常数初始化通常效果不佳，太固定了。随机初始化一般是均匀分布和正态分布。</p> <ul><li>Xavier初始化和Kaiming 初始化<br>
大体思想是认为神经网络的每一层方差大致不变，因此如果某一层神经元个数偏少，则有可能方偏小，则需要在初始化进行弥补。</li></ul> <p>自定义初始化可以定义一个函数，举个栗子：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>my_mlp <span class="token operator">=</span> MLP<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#实例化一个MLP</span>
<span class="token keyword">def</span> <span class="token function">weights_init_uniform</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
   classname <span class="token operator">=</span> m<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__
   <span class="token keyword">if</span> classname<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'Linear'</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span> <span class="token punctuation">:</span><span class="token comment">#比如只对线性层初始化</span>
        m<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">)</span>
        m<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
my_mlp<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>weights_init_uniform<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h2 id="学习率"><a href="#学习率" class="header-anchor">#</a> 学习率</h2> <ul><li>常见trick：</li></ul> <ol><li>首先寻找ok的学习率，然后再调整其它参数</li> <li>不同层采用不同的学习率(上下层训练程度不一样就采用不同学习率)</li> <li>在最终阶段降低学习率，或者Babysit（要去观测整个训练，当发现网络在验证集上的精度已经不再提升了就开始逐步降低学习率）</li></ol> <ul><li><p>常见的学习率<br>
Finetune: 1e-5,2e-5,5e-5<br>
重新训练，没有公认的介定，一般从0.01开始尝试</p></li> <li><p>warm up<br>
由于刚开始训练的时候学习率太大会导致不稳定，但是有最新论文提出刚开始如果学习率太小也会不稳定，因此使用一个折中的办法warm up。warm up是学习率从小到大再到小。<br>
学习率设定的栗子：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>  optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span>mlp<span class="token punctuation">.</span>first_layer<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'lr'</span><span class="token punctuation">:</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
                                   <span class="token string">'params'</span><span class="token punctuation">:</span>mlp<span class="token punctuation">.</span>second_layer<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'lr'</span><span class="token punctuation">:</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">2e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># adm optimizer当中也是有信息的，如果需要连续训练，则也需要保存再读取，但是读取厚还是需要重新设置一下学习率</span>
   torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'optimizer.pt'</span><span class="token punctuation">)</span>
   optimizer2 <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span>mlp<span class="token punctuation">.</span>first_layer<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'lr'</span><span class="token punctuation">:</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
                                   <span class="token string">'params'</span><span class="token punctuation">:</span>mlp<span class="token punctuation">.</span>second_layer<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'lr'</span><span class="token punctuation">:</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">2e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span> 
   optimizer2<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'optimizer.pt'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>          
   optimizer2<span class="token punctuation">.</span>__dict__
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div></li></ul> <p>warm up的栗子：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>params <span class="token operator">=</span> mlp<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
n_epoch <span class="token operator">=</span> <span class="token number">3</span>
global_steps <span class="token operator">=</span> <span class="token number">0</span>
warm_up_steps <span class="token operator">=</span> <span class="token number">1000</span>
max_learninig_rate <span class="token operator">=</span> <span class="token number">0.01</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>
     <span class="token keyword">for</span> batch <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
     global_steps <span class="token operator">+=</span> <span class="token number">1</span>
     optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
     x<span class="token punctuation">,</span> y <span class="token operator">=</span> batch
     predictions <span class="token operator">=</span> mlp<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
     loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
     loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
     optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
     <span class="token keyword">if</span> global_steps <span class="token operator">&lt;</span> <span class="token number">1000</span><span class="token punctuation">:</span>
         optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span> <span class="token operator">=</span> global_steps<span class="token operator">*</span> max_learning_rate<span class="token operator">/</span>warm_up_steps
    <span class="token keyword">else</span><span class="token punctuation">:</span>
         optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span> <span class="token operator">=</span> max_learning_rate<span class="token comment"># 这里大于1000步不变，实际也可以使学习率再降低</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><h2 id="梯度累积"><a href="#梯度累积" class="header-anchor">#</a> 梯度累积</h2> <p>当显存不够时，不能用较大的batchsize。<br>
总结来说：梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空，不断累加，累加一定次数后，根据累加的梯度更新网络参数，然后清空梯度，进行下一次循环。一定条件下，batchsize越大训练效果越好，梯度累加则实现了batchsize的变相扩大，如果accumulation_steps为8，则batchsize '变相' 扩大了8倍，是我们这种乞丐实验室解决显存受限的一个不错的trick，使用时需要注意，学习率也要适当放大。</p> <h2 id="分布式训练"><a href="#分布式训练" class="header-anchor">#</a> 分布式训练</h2> <p>多卡，传输数据</p> <h2 id="半精度训练"><a href="#半精度训练" class="header-anchor">#</a> 半精度训练</h2> <p>float16</p></div> <footer class="page-edit" style="display:none;"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">2/22/2023, 6:05:51 AM</span></div></footer> <!----> <!----> <!----></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/assets/js/app.c903a5b0.js" defer></script><script src="/assets/js/3.20f81737.js" defer></script><script src="/assets/js/1.0bcafa9d.js" defer></script><script src="/assets/js/41.3b6c6e38.js" defer></script>
  </body>
</html>
