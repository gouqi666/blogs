---
title: 集成树模型
date: 2021-04-06
sidebar: "auto"
categories:
  - mlAndMath
tags:
  - tree
  - dl
---
## 树模型概述
-  优点
     - 非线性效应
     - 交叉效应
     - 稀疏，可以处理高维变量
- 缺点
    - 不稳定 （边缘效应）
    - 表现力差（强行增加深度，结果也并不是很好）
    - 精度差
- 集成树
   改变树的上述缺点 
   - 随机森林类
      - 随机森林
            每次拟合一个新的树的时候，随机抽一些变量和观测，重新拟合树。
      -  ExtraTrees
          对于随机的变量和观测取一个分割点，取一个极限，防止过拟合。
   - 梯度提升树： GBDT, XGBoost, LightGBM, CatBoost等
   
   
## 常见的集成策略
  - Bagging模型   
    就像它的名字一样，bagging模型会把(相同的)若干基础模型简单的“装起来”——基础模型独立训练，然后将它们的输出用特定的规则综合(比如求平均值)起来，形成最后的预测值。最常见的bagging模型是随机森林(Random Forest)。在回归类任务中，bagging模型假设各个基础模型的预测值“错落有致”，分布在真实值的周围——把这些预测值平均一下，就可以稳定地得到一个比较准确的预测值;而在分类任务中，bagging模型认为“每一个基础模型都判断错误”发生的概率是比较低的，基础模型中的相当一部分会做出正确的判断，因此可以基于大家的投票结果来选择最终类别。
  - Stacking模型  
  Stacking模型比bagging模型更进2步：(a)允许使用不同类型的模型作为base model；（b）使用一个机器学习模型把所有base model的输出汇总起来，形成最终的输出。（b）所述的模型被称为“元模型”。在训练的时候，base model们直接基于训练数据独立训练，而元模型会以它们的输出为输入数据、以训练数据的输出为输出数据进行训练。Stacking模型认为，各个基础模型的能力不一，投票的时候不能给以相同的权重，而需要用一个“元模型”对各个基础模型的预测值进行加权。
  - Boosting模型  
     Boosting模型采用另一种形式，把基础模型组合起来——串联。这类模型的思想是，既然一个基础模型可以做出不完美的预测，那么我们可以用第二的基础模型，把“不完美的部分”补上。我们可以使用很多的基础模型，不断地对“不完美的部分”进行完善，以得到效果足够好的集成模型。Boosting的策略非常多，以GBDT为例，它会用第K个CART拟合前k-1个CART留下的残差，从而不断的缩小整个模型的误差，如图2-4。
       
## 具体的模型详解
   - GBDT（Gradient Boosting Decision Tree）  
    基本原理是构建n个决策树（这里一般是回归树），每个决策树输出的值Tk(x)，但是每次的最终输出是$F_{k}(x)$（是前n-1个决策树输出的值加上当前他$T_{k}(x)$）, 用均方误差当作损失函数，这时候计算Loss对$F_{k-1}$的梯度，这样用类似于梯度下降的方式，下一次优化使$F_{k}(x)$向负梯度方向优化，这样就可以得出$T_{k}(x) = y_{n} - F_{k-1}(x_{n})$,这里因为使用的是均方误差损失函数，导致负梯度和残差是表达式想同，所以有一种拟合残差的思想，但是换一个损失函数就不是了，因此通用的来说还是拟合负梯度，因为有可能还有正则惩罚项，导致完全拟合残差也不是损失函数下降最快的方向。  
    [GBDT相关链接1](https://zhuanlan.zhihu.com/p/144855223)  
    [GBDT相关链接2](https://blog.csdn.net/zhaojc1995/article/details/88177939)
  - XgBoost(eXtreme Gradient Boosting)
     <img :src="$withBase('/notes/treeModel1.jpg')" alt="treeModel">
     <img :src="$withBase('/notes/treeModel2.jpg')" alt="treeModel2">

    Xgboost是GBDT算法的一种很好的工程实现，并且在算法上做了一些优化，主要的优化在一下几点。首先Xgboost采用二阶泰勒展开拟合损失函数，可以加速模型收敛；然后加了一个衰减因子，相当于一个学习率，可以减少加进来的树对于原模型的影响，让树的数量变得更多；其次是在原GBDT模型上加了个正则项，对于树的叶子节点的权重做了一个约束；还有增加了在随机森林上常用的col subsample的策略；然后最大的地方在于不需要遍历所有可能的分裂点了，它提出了一种分位数估计分裂点的算法。在工程上做了一个算法的并发实现，具体我并不了解如何实现的  
          
      - XgBoost参数  
        最重要的参数是树的深度    
        eta:一般是0.01-0.2(学习率）  
        min_child_weight: 在这一支中大约有多少个观测，观测过少，则有可能出现每一个观测分一个叶子结点的情况，这样就会出现过拟合，控制这个参数防止过拟合。  
        gamma: 乘以树结点数量的参数      
        Dart模式：建议选择为True, 跟dropout很像，算第n颗树的时候，抛弃一些树计算，防止过拟合  
        树的个数也是由少到多测试。    
- LightGBM  
    &nbsp;&nbsp;出发点是为了提高GBDT的运行效率，但是实际上也提高了GBDT的运行效率。GBDT在计算的时候要求所有观测值的梯度，实际上某些观测值的梯度很小或者考虑数据有随机性的时候，并不一定需要全部计算，甚至可以看成是噪音。  
    
    &nbsp;&nbsp;如果一个样本的梯度较小，证明这个样本训练的误差已经很小了，所以不需要计算了。我们在XGB的那篇文章中说过，GBDT的梯度算出来实际上就是残差，梯度小残差就小，所以该样本拟合较好，不需要去拟合他们了。这听起来仿佛很有道理，但问题是丢掉他们会改变数据的分布，还是无法避免信息损失，进而导致精度下降，所以LGB提出了一个很朴实无华且枯燥的方法进行优化。
&nbsp;&nbsp;LGB的优化方法是，在保留大梯度样本的同时，随机地保留一些小梯度样本，同时放大了小梯度样本带来的信息增益。这样说起来比较抽象，我们过一遍流程： 首先把样本按照梯度排序，选出梯度最大的a%个样本，然后在剩下小梯度数据中随机选取b%个样本，在计算信息增益的时候，将选出来b%个小梯度样本的信息增益扩大 1 - a / b 倍。这样就会避免对于数据分布的改变。  
&nbsp;&nbsp;除了通过部分样本计算信息增益以外，LGB还内置了特征降维技术，思想就是合并那些冲突小的稀疏特征。举个例子，对于一列特征[1,nan,1,nan,1]和一列特征[nan,1,nan,1,nan]，他们正好可以合并成一列特征[1,2,1,2,1]。LGB的目标就是在于找到这样的特征并且将他们合并在一起。如果把特征抽象成图中的点，特征之间的冲突看作是图中的边，那么问题就转换为找出图中的社团并使图中的社团数量最少。LGB里提出了一个贪心的策略，按照有权度来为图中所有的点排序，然后把特征合并到度小于某个阈值的社团中或单独创建一个社团。对于特征如何合并，一个重要的原则就是使合并的两个特征可以被顺利区分出来，LGB采取了一个更改阈值的方法。例如对于特征x∈(0, 10)， 特征y∈(0, 20)，就可以把特征y转换为y∈(10,30)，然后再去合并x与y。
&nbsp;&nbsp;  
[LGB的详细文章](https://zhuanlan.zhihu.com/p/89360721)
   - 参数
      查看官方文档，很详细。
 - CatBoost && NGBoost
     - CatBoost
          1. 只接受离散的变量，不要进行one-hot编码，核心思想是Ordered Target Mean Encoding,参数有深度，数量，学习率等。  
          2. 嵌入了自动将类别型特征处理为数值型特征的创新算法。首先对categorical features做一些统计，计算某个类别特征（category）出现的频率，之后加上超参数，生成新的数值型特征（numerical features）。
          3. Catboost还使用了组合类别特征，可以利用到特征之间的联系，这极大的丰富了特征维度。
          4. 采用排序提升的方法对抗训练集中的噪声点，从而避免梯度估计的偏差，进而解决预测偏移的问题。
          5. 采用了完全对称树作为基模型。
     - NGBoost  
        与GBDT类似，用Natural Gradient替代原始的梯度，Natural Gradient实际上为一阶导数除以二阶导数（和牛顿法类似），计算成本很高。
  - 集成树模型总结  
      一般来说，用XgBoost和LightGBM作为初始分类器，其余可作为补充 。      
      特征工程的效果大于调参。