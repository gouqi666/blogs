---
title: meta learning
date: 2021-11-25
sidebar: "auto"
categories:
  - notes
tags:
  - meta learning
---

## meta learning 是最近比较火的一个研究方向，这里大致记录一下其思想。

## 思想
    元学习Meta Learning，含义为学会学习，即learn to learn，就是带着这种对人类这种“学习能力”的期望诞生的。Meta Learning希望使得模型获取一种“学会学习”的能力，使其可以在获取已有“知识”的基础上快速学习新的任务，如：让Alphago迅速学会下象棋，让一个猫咪图片分类器，迅速具有分类其他物体的能力。通常在机器学习里，我们会使用某个场景的大量数据来训练模型；然而当场景发生改变，模型就需要重新训练。但是对于人类而言，一个小朋友成长过程中会见过许多物体的照片，某一天，当Ta（第一次）仅仅看了几张狗的照片，就可以很好地对狗和其他物体进行区分。  
## 与传统的机器学习不同的地方
- 传统的机器学习是为了求解某个具体的task或者多个task的最优解，其输入是某个task的训练集，验证集，测试集。在机器学习中，训练单位是一条数据，通过数据来对模型进行优化；数据可以分为训练集、测试集和验证集。在元学习中，训练单位分层级了，第一层训练单位是任务，也就是说，元学习中要准备许多任务来进行学习，第二层训练单位才是每个任务对应的数据。  
- meta learning的输入可以理解是多个任务，这些任务之间是带有一定相关性的，满足某种分布。同时其可以直接应用于其它任务上，使得其它任务训练得更快更好（如果目标任务数据集很少的话，则可以很好的解决这个问题）。它的输出是一个模型或者说函数，它的输出就能用于其它任务。  
- 二者的目的都是找一个Function，只是两个Function的功能不同，要做的事情不一样。机器学习中的Function直接作用于特征和标签，去寻找特征与标签之间的关联；而元学习中的Function是用于寻找新的f，新的f才会应用于具体的任务。    
- meta learning其原则是可以自己构建新的模型和初始化参数，但是构建新模型的方式太过复杂了，我还没有看，下面会举个比较基础的算法（MAML）来构建新的初始化参数。  
## loss
我们需要注意下meta learning的loss与传统的机器学习loss不同。    

<img :src="$withBase('/notes/meta1.png')" alt="meta1">
    
**这里用公式不太好表述，N代表N个任务，l表示loss，Φ表示当前模型的参数，θ^n 表示在第n个任务上的参数，什么意思呢？**  
其大致思想就是说meta learning不关注某个具体任务上的训练集loss，比如我现在有很多分类任务，如区分猫狗，区分非机动车与机动车，区分人与动物等等任务，当前模型参数是Φ，我随机sample N个任务，用Φ去当作 模型起始参数，在每个任务上分别训练（其原论文说的训练一次，具体实现时又不是一次，所以暂不纠结这个）几次，然后去验证集上计算loss，将这个loss反向传播到meta learning模型中去，然后再累积多任务求和更新Φ，从这里就可以看出来这个Φ不一定在每个问题上都是最优的，但是其目的是想得到一个Φ，然后应用于某个具体任务时，只需要训练少量的step就可以使其达到最优，这点也是和transfer learning 区别最大的地方。

## MAML
- MAML的目的是获取一组更好的模型初始化参数（即让模型自己学会初始化）。我们通过许多训练任务进行元学习的训练，使得模型学习到“先验知识”（初始化的参数）。  
其算法流程图如下：其思想和上面叙述的差不多。
<img :src="$withBase('/notes/meta2.png')" alt="meta2">

    